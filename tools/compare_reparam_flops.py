#!/usr/bin/env python
"""
å¯¹æ¯” BN é‡å‚æ•°åŒ–å‰åçš„ FLOPs å’Œå‚æ•°é‡
"""
import argparse
import sys
import os
from pathlib import Path

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import torch
from mmengine.config import Config
from mmengine.logging import MMLogger
from mmengine.model import revert_sync_batchnorm
from mmengine.registry import init_default_scope
from mmdet.registry import MODELS

#  å…³é”®ï¼šå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ä»¥ç¡®ä¿æ³¨å†Œåˆ° MODELS
import seg.models.backbones  # æ³¨å†Œ RegNetReparam
import seg.models.hooks  # æ³¨å†Œ ReparamHook
import seg.models.detectors  # æ³¨å†Œè‡ªå®šä¹‰ detectors

try:
    from mmengine.analysis import get_model_complexity_info
    from mmengine.analysis.print_helper import _format_size
except ImportError:
    raise ImportError('Please upgrade mmengine >= 0.6.0')


def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params


def count_bn_layers(model, prefix=''):
    """ç»Ÿè®¡ BN å±‚æ•°é‡"""
    bn_count = 0
    bn_params = 0
    
    for name, module in model.named_modules():
        if isinstance(module, (torch.nn.BatchNorm2d, torch.nn.BatchNorm1d, torch.nn.SyncBatchNorm)):
            bn_count += 1
            # BN å±‚å‚æ•°: weight + bias + running_mean + running_var
            if hasattr(module, 'weight') and module.weight is not None:
                bn_params += module.weight.numel()  # gamma
            if hasattr(module, 'bias') and module.bias is not None:
                bn_params += module.bias.numel()  # beta
            if hasattr(module, 'running_mean') and module.running_mean is not None:
                bn_params += module.running_mean.numel()
            if hasattr(module, 'running_var') and module.running_var is not None:
                bn_params += module.running_var.numel()
    
    return bn_count, bn_params


def analyze_model(config_path, use_reparam=False):
    """åˆ†ææ¨¡å‹çš„ FLOPs å’Œå‚æ•°é‡"""
    logger = MMLogger.get_instance(name='MMLogger')
    
    cfg = Config.fromfile(config_path)
    init_default_scope(cfg.get('default_scope', 'mmdet'))
    
    # æ„å»ºæ¨¡å‹
    model = MODELS.build(cfg.model)
    if torch.cuda.is_available():
        model = model.cuda()
    model = revert_sync_batchnorm(model)
    model.eval()
    
    # ç»Ÿè®¡èåˆå‰çš„ä¿¡æ¯
    total_params_before, trainable_params_before = count_parameters(model)
    bn_count_before, bn_params_before = count_bn_layers(model)
    
    print(f"\n{'='*60}")
    print(f"{'èåˆå (Reparameterized)' if use_reparam else 'èåˆå‰ (Original)'}")
    print(f"{'='*60}")
    
    # å¦‚æœéœ€è¦é‡å‚æ•°åŒ–ï¼Œæ‰‹åŠ¨è°ƒç”¨èåˆ
    if use_reparam:
        if hasattr(model, 'backbone'):
            backbone_type = type(model.backbone).__name__
            print(f"\n Backbone ç±»å‹: {backbone_type}")
            print(f" Backbone ç±»: {type(model.backbone)}")
            print(f" å¯ç”¨æ–¹æ³•: {[m for m in dir(model.backbone) if 'switch' in m.lower() or 'deploy' in m.lower() or 'reparam' in m.lower()]}")
            
            if hasattr(model.backbone, 'switch_to_deploy'):
                print("\n æ‰§è¡Œ BN èåˆ...")
                model.backbone.switch_to_deploy()
                print(" BN èåˆå®Œæˆï¼\n")
            else:
                print(f"\n Backbone ({backbone_type}) ä¸æ”¯æŒé‡å‚æ•°åŒ–\n")
        else:
            print("\n æ¨¡å‹æ²¡æœ‰ backbone å±æ€§\n")
    
    # ç»Ÿè®¡èåˆåçš„ä¿¡æ¯
    total_params_after, trainable_params_after = count_parameters(model)
    bn_count_after, bn_params_after = count_bn_layers(model)
    
    # è®¡ç®— FLOPs (ä½¿ç”¨å›ºå®šè¾“å…¥å¤§å°)
    input_shape = (640, 640)  # ä»é…ç½®æ–‡ä»¶ä¸­çš„ image_size è¯»å–
    inputs = torch.randn(1, 3, *input_shape)
    if torch.cuda.is_available():
        inputs = inputs.cuda()
    
    try:
        outputs = get_model_complexity_info(
            model,
            input_shape=input_shape,
            inputs=inputs,
            show_table=False,
            show_arch=False
        )
        flops = outputs['flops']
        params = outputs['params']
        flops_str = _format_size(flops)
        params_str = _format_size(params)
    except Exception as e:
        print(f" FLOPs è®¡ç®—å¤±è´¥: {e}")
        flops_str = "N/A"
        params_str = f"{total_params_after / 1e6:.3f}M"
    
    # æ‰“å°ç»“æœ
    print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
    print(f"  - æ€»å‚æ•°é‡: {params_str} ({total_params_after:,} ä¸ª)")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params_after / 1e6:.3f}M ({trainable_params_after:,} ä¸ª)")
    print(f"  - BN å±‚æ•°é‡: {bn_count_after} ä¸ª")
    print(f"  - BN å‚æ•°é‡: {bn_params_after / 1e3:.2f}K ({bn_params_after:,} ä¸ª)")
    print(f"\nğŸ“ˆ è®¡ç®—é‡:")
    print(f"  - FLOPs: {flops_str}")
    print(f"  - è¾“å…¥å°ºå¯¸: {input_shape}")
    
    return {
        'total_params': total_params_after,
        'trainable_params': trainable_params_after,
        'bn_count': bn_count_after,
        'bn_params': bn_params_after,
        'flops': flops_str,
    }


def main():
    parser = argparse.ArgumentParser(description='å¯¹æ¯” BN é‡å‚æ•°åŒ–å‰åçš„æ¨¡å‹æ€§èƒ½')
    parser.add_argument('config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # åˆ†æèåˆå‰çš„æ¨¡å‹
    print("\n" + "ğŸ” æ­£åœ¨åˆ†æåŸå§‹æ¨¡å‹ï¼ˆæœªèåˆ BNï¼‰...")
    results_before = analyze_model(args.config, use_reparam=False)
    
    # åˆ†æèåˆåçš„æ¨¡å‹
    print("\n" + "ğŸ” æ­£åœ¨åˆ†æé‡å‚æ•°åŒ–æ¨¡å‹ï¼ˆèåˆ BNï¼‰...")
    results_after = analyze_model(args.config, use_reparam=True)
    
    # å¯¹æ¯”ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“Š é‡å‚æ•°åŒ–æ•ˆæœå¯¹æ¯”")
    print(f"{'='*60}")
    
    param_reduction = results_before['total_params'] - results_after['total_params']
    bn_count_reduction = results_before['bn_count'] - results_after['bn_count']
    bn_param_reduction = results_before['bn_params'] - results_after['bn_params']
    
    print(f"\n å‚æ•°é‡å‡å°‘:")
    print(f"  - æ€»å‚æ•°å‡å°‘: {param_reduction / 1e3:.2f}K ({param_reduction:,} ä¸ª)")
    print(f"  - BN å±‚å‡å°‘: {bn_count_reduction} ä¸ª")
    print(f"  - BN å‚æ•°å‡å°‘: {bn_param_reduction / 1e3:.2f}K ({bn_param_reduction:,} ä¸ª)")
    
    if param_reduction > 0:
        reduction_rate = (param_reduction / results_before['total_params']) * 100
        print(f"  - å‚æ•°é‡å‡å°‘æ¯”ä¾‹: {reduction_rate:.2f}%")
    
    print(f"\nğŸ’¡ ç»“è®º:")
    print(f"  - BN èåˆåï¼Œå‚æ•°é‡ä» {results_before['total_params']/1e6:.3f}M é™è‡³ {results_after['total_params']/1e6:.3f}M")
    print(f"  - æ¨ç†é€Ÿåº¦å°†æå‡ï¼ˆå‡å°‘äº† {bn_count_reduction} ä¸ª BN å±‚çš„è®¡ç®—ï¼‰")
    print(f"  - æ›´é€‚åˆéƒ¨ç½²åˆ° NPU/ç§»åŠ¨ç«¯è®¾å¤‡")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    main()

